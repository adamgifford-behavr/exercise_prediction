
Steps:
1. choose data set that would require streaming
2. do simple data exploration (w/ cleaning if necessary)
3. build data cleaning pipeline (includes featurization)
4. test 3 model types using hyperopt and MLflow (promoting one through to production)
	- use localstack to mimic s3 (and rds if possible, otherwise use local folder for artifact store/model registry)
5. create prefect flow to periodically gather data, run it through pipeline, train model, compare against previous,
promote new if better
6. deploy model
	- use localstack to mimic kinesis (and lambda if necessary and possible)
7. monitor performance with prometheus, evidently, grafana
8. code testing
	- pytest
	- linting/formatting
	- integration tests
	- git pre-commit hooks
	- makefile and make