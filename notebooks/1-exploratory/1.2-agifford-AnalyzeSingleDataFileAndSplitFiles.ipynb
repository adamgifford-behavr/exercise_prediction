{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2-agifford-AnalyzeSingleDataFileAndSplitFiles\n",
    "This notebook performs exploratory data analysis on an example datafile. Specifically, we identify peak frequencies as a function of activity label and attempt to determine an appropriate threshold level on the magnitude of the peaks to identify those frequencies to use as features in a model for predicting activity. We want a threshold that yields only ~1-2 peak frequencies per activity (not including the 0-frequency average).\n",
    "\n",
    "This notebook also templates out code to sort raw parquet files into train/val vs. test data sets so that, moving forward, we will only explore files within the training set and use the validation set strictly for tweaking the model and the test set strictly for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft, fftshift\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "parq_file = \"../../data/interim/raw/fileID1_subjID3_dataID0.parquet\"\n",
    "df = pd.read_parquet(parq_file, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_single_annot_frame(df, shift):\n",
    "    annot_df = df[df.label != df.label.shift(shift)]\n",
    "    annot_df = annot_df.dropna(subset=\"label\").reset_index()\n",
    "    return annot_df\n",
    "\n",
    "def make_annot_dataframe(df, t_start=None, t_end=None):\n",
    "    t_start = t_start or df.time.min()\n",
    "    t_end = t_end or df.time.max()\n",
    "    \n",
    "    df = df[(df.time >= t_start) & (df.time <= t_end)].copy()\n",
    "    \n",
    "    (act_starts_df, act_ends_df) = (\n",
    "        _make_single_annot_frame(df, shift) for shift in [1, -1]\n",
    "    )\n",
    "    return act_starts_df, act_ends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure I'll need the starts & ends df, but probably should remove the rows with no \n",
    "# activity labels\n",
    "activity_starts_df, activity_ends_df = make_annot_dataframe(df)\n",
    "df_dropna =  df.dropna(subset=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df_dropna.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of getting through this project end-to-end, I will not spend too much time on building a very sophisticated model. As such, I will use the column `label_group` as my desired prediction column.\n",
    "\n",
    "Process for each `label_group`:\n",
    "1. Compute FFT with a Hanning window for each instance of the `label_group`\n",
    "2. Average the FFTs across instances\n",
    "3. Identify the major frequencies of the group by setting some arbitrary threshold to identify peaks.\n",
    "4. I will use those frequencies to generate `sin` and `cos` features as inputs to a basic model to predict `label_group`.\n",
    "5. Repeat steps 1-4 for each variable x direction combination (e.g., \"accel_x\", \"accel_y\", etc.)\n",
    "\n",
    "First, let's template out the process of analyzing a single instance of a single `label_group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_tp_df = pd.concat([activity_starts_df.head(1), activity_ends_df.head(1)], ignore_index=True)\n",
    "df_snip = df_dropna[(df_dropna.time >= act_tp_df.loc[0, \"time\"]) & (df_dropna.time <= act_tp_df.loc[1, \"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 50\n",
    "n_fft = df_snip.shape[0]\n",
    "window = signal.hann(n_fft)\n",
    "X_w = fft(window * df_snip.accel_x.values)\n",
    "n_points = 2 * int(np.floor(n_fft / 2))\n",
    "if n_fft % 2:\n",
    "    n_points += 1\n",
    "freq = fs/2 * np.linspace(-1, 1, n_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing strong in \"\\<Initial Activity\\>\" except for 0 Hz..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_w_norm = np.abs(fftshift(X_w))\n",
    "X_w_norm = 20 * np.log10(np.abs(fftshift(X_w / abs(X_w).max())))\n",
    "plt.plot(freq, X_w_norm)\n",
    "plt.title(\"Frequency response first activity\")\n",
    "plt.ylabel(\"Normalized magnitude [dB]\")\n",
    "plt.xlabel(\"F [Hz]\")\n",
    "print(act_tp_df.loc[0, \"label\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_tp_df = pd.concat([activity_starts_df.loc[[4], :], activity_ends_df.loc[[4], :]], ignore_index=True)\n",
    "df_snip = df_dropna[(df_dropna.time >= act_tp_df.loc[0, \"time\"]) & (df_dropna.time <= act_tp_df.loc[1, \"time\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, there seem to be many prevalent peaks in \"Jumping Jacks\" at ~1 Hz and 2.75Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 50\n",
    "n_fft = df_snip.shape[0]\n",
    "window = signal.hann(n_fft)\n",
    "X_w = fft(window * df_snip.accel_x.values)\n",
    "n_points = 2 * int(np.floor(n_fft / 2))\n",
    "if n_fft % 2:\n",
    "    n_points += 1\n",
    "freq = fs/2 * np.linspace(-1, 1, n_points)\n",
    "\n",
    "# X_w_norm = np.abs(fftshift(X_w))\n",
    "X_w_norm = 20 * np.log10(np.abs(fftshift(X_w / abs(X_w).max())))\n",
    "\n",
    "a = np.diff(np.sign(np.diff(X_w_norm))).nonzero()[0] + 1               # local min & max\n",
    "b = (np.diff(np.sign(np.diff(X_w_norm))) > 0).nonzero()[0] + 1         # local min\n",
    "c = (np.diff(np.sign(np.diff(X_w_norm))) < 0).nonzero()[0] + 1         # local max\n",
    "# +1 due to the fact that diff reduces the original index number\n",
    "\n",
    "plt.plot(freq, X_w_norm, color=\"grey\")\n",
    "plt.plot(freq, [-10 for _ in X_w_norm], color=\"orange\")\n",
    "plt.plot(freq[b], X_w_norm[b], \"o\", label=\"min\", color='r')\n",
    "plt.plot(freq[c], X_w_norm[c], \"o\", label=\"max\", color='b')\n",
    "plt.title(\"Frequency response first activity\")\n",
    "plt.ylabel(\"Normalized magnitude [dB]\")\n",
    "plt.xlabel(\"F [Hz]\")\n",
    "plt.xlim([0, 5])\n",
    "print(act_tp_df.loc[0, \"label\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a function that pulls out the max of the peaks that cross the threshold (i.e., just gets the 0.02, 0.95, and 2.73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_fmax_above_thresh(freq, x_w, threshold):\n",
    "    local_max_ix = (np.diff(np.sign(np.diff(x_w))) < 0).nonzero()[0] + 1\n",
    "    x_w_max = x_w[local_max_ix]\n",
    "    freq_max = freq[local_max_ix]\n",
    "\n",
    "    return freq_max[np.where((x_w_max>threshold) & (freq_max>0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fmax_above_thresh(freq, X_w_norm, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to cycle through all of the activities, and extract the peak frequencies above a particular threshold. What I want to find is an \"ideal\" threshold such that I'm only picking out 2 peak frequencies (3 including 0 Hz) for the majority of activities. This will be the threshold I work with for the rest of the project to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normed_spectrum(df, fs=50):\n",
    "    n_fft = df.shape[0]\n",
    "    window = signal.hann(n_fft)\n",
    "    X_w = fft(window * df.accel_x.values)\n",
    "    X_w_norm = 20 * np.log10(np.abs(fftshift(X_w / abs(X_w).max())))\n",
    "\n",
    "    n_points = 2 * int(np.floor(n_fft / 2))\n",
    "    if n_fft % 2:\n",
    "        n_points += 1\n",
    "    freq = fs/2 * np.linspace(-1, 1, n_points)\n",
    "    return X_w_norm, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [-5, -10, -15, -20]\n",
    "all_pks_df = pd.DataFrame(columns=[\"threshold\", \"activity\", \"peak_fs\"])\n",
    "for thresh in thresholds:\n",
    "    for r_ix in range(activity_starts_df.shape[0]):\n",
    "        act_tp_df = pd.concat([activity_starts_df.loc[[r_ix], :], activity_ends_df.loc[[r_ix], :]], ignore_index=True)\n",
    "        df_snip = df_dropna[(df_dropna.time >= act_tp_df.loc[0, \"time\"]) & (df_dropna.time <= act_tp_df.loc[1, \"time\"])]\n",
    "        X_w_norm, freq = calculate_normed_spectrum(df_snip)\n",
    "\n",
    "        local_fmax = local_fmax_above_thresh(freq, X_w_norm, thresh)\n",
    "        data = {\n",
    "            \"threshold\": [thresh for _ in local_fmax],\n",
    "            \"activity\": [act_tp_df.loc[0, \"label\"] for _ in local_fmax],\n",
    "            \"peak_fs\": local_fmax\n",
    "        }\n",
    "        pks_df = pd.DataFrame(data=data)\n",
    "        all_pks_df = pd.concat([all_pks_df, pks_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pks_df[\"q_rounded_fs\"] = all_pks_df[\"peak_fs\"].apply(lambda x: np.round(x * 4) / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pks_df.q_rounded_fs.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pks_df[\"h_rounded_fs\"] = all_pks_df[\"peak_fs\"].apply(lambda x: np.round(x * 2) / 2)\n",
    "all_pks_df.h_rounded_fs.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like simply going to full-rounded frequencies may be the way to go to limit the total number of features I'll need to generate across activities. I'll test it across datasets to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pks_df[\"rounded_fs\"] = all_pks_df[\"peak_fs\"].apply(lambda x: np.round(x))\n",
    "np.sort(all_pks_df.rounded_fs.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, I should separate out a test dataset from the set of raw parquet files. There are 126 files, and we want an 80/20 split, so `126 * 0.2 ~= 25` files should be designated for the test set. There are also 94 subjects, and some users should only be included in the test set (to better ensure generalizability of the models), so `94 * 0.2 ~= 19` subjects should be included in the test set. In order to achieve this split, I will find 13 subjects with 1 exercise run, and 6 subjects with 2 exercise runs. This will give me `13 * (1) + 6 * (2) = 13 + 12 = 25` total files in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_dir = Path(\"../../data/interim/raw/\")\n",
    "all_files = list(x for x in parquet_dir.iterdir() if x.is_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_files_dict(all_files):\n",
    "    files_dict = defaultdict(list)\n",
    "    for file in all_files:\n",
    "        _, subj, _ = file.parts[-1].split(\"_\")\n",
    "        files_dict[subj].append(str(file))\n",
    "    \n",
    "    return files_dict\n",
    "\n",
    "def _get_first_subjs_match_crit(files_dict, n_sing_file, n_double_file):\n",
    "    test_subj_ids = []\n",
    "    ones_left = n_sing_file\n",
    "    twos_left = n_double_file\n",
    "    for key, val in files_dict.items():\n",
    "        if not (ones_left or twos_left):\n",
    "            break\n",
    "\n",
    "        if (len(val) == 1) & (ones_left > 0):\n",
    "            test_subj_ids.append(key)\n",
    "            ones_left -= 1\n",
    "        elif (len(val) == 2) & (twos_left > 0):\n",
    "            test_subj_ids.append(key)\n",
    "            twos_left -= 1\n",
    "\n",
    "    return test_subj_ids\n",
    "\n",
    "def _make_train_test_dict(all_files, test_subj_ids):\n",
    "    train_test_files = defaultdict(list)\n",
    "    for file in all_files:\n",
    "        _, subj, _ = file.parts[-1].split(\"_\")\n",
    "        if any([subj==test_subj for test_subj in test_subj_ids]):\n",
    "            train_test_files[\"test\"].append(str(file))\n",
    "        else:\n",
    "            train_test_files[\"train_val\"].append(str(file))\n",
    "    \n",
    "    return train_test_files\n",
    "\n",
    "def make_train_test_split_json(interim_path):\n",
    "    all_files = list(x for x in interim_path.iterdir() if x.is_file())\n",
    "    files_dict = _make_files_dict(all_files)\n",
    "    \n",
    "    test_subj_ids = _get_first_subjs_match_crit(files_dict, 13, 6)\n",
    "\n",
    "    train_test_files = _make_train_test_dict(all_files, test_subj_ids)\n",
    "    \n",
    "    with open(\"../../src/data/train-val_test.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(train_test_files, outfile)\n",
    "    \n",
    "    return files_dict, test_subj_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_files, test_subj_ids = make_train_test_split_json(parquet_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions check out, so we have our list of test subjects, for which all fileIDs will be included in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = sum([len(subj_files[key]) for key in test_subj_ids])\n",
    "assert len(test_subj_ids) == 19\n",
    "assert n_files == 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use this same methodology to split the training/validation files into strict training vs. validation datasets. We'll use the same 80/20 split. Given that there are 101 files, we want `101 * 0.2 ~= 20` files saved for the validation set. Additionally, since there are 75 subjects in the train/val set, we want `75 * 0.2 = 15` subjects in the validation set. I will attempt to proportionally split out the number of subjects with large (i.e., more than 1) data sets by hand based on this split, and then fill in the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_val_split_json(train_val_test_path, n_val_subjs, n_val_files, n_files_tol=1, **kwargs):\n",
    "    default_ns = {\n",
    "        \"n_5_files\": 0,\n",
    "        \"n_4_files\": 1,\n",
    "        \"n_3_files\": 1,\n",
    "        \"n_2_files\": 1 \n",
    "    }\n",
    "    val_ns_files = {\n",
    "        key: val if key not in kwargs.keys() else kwargs[key]\n",
    "        for key, val in default_ns.items()\n",
    "    }\n",
    "    val_ns_files.update(\n",
    "        {key: val for key, val in kwargs.items() if key not in val_ns_files.keys()}\n",
    "    )\n",
    "\n",
    "    with open(train_val_test_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "        train_test_files = json.load(infile)\n",
    "\n",
    "    train_val_files = [Path(file) for file in train_test_files[\"train_val\"]]\n",
    "    \n",
    "    files_dict = _make_files_dict(train_val_files)\n",
    "\n",
    "\n",
    "    ns_dict = {\n",
    "        key: len(val) for key, val in files_dict.items()\n",
    "    }\n",
    "    sort_subjs_ns = sorted(ns_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    sort_subjs, sorted_ns = (\n",
    "        [s[0] for s in sort_subjs_ns],\n",
    "        [s[1] for s in sort_subjs_ns],\n",
    "    )\n",
    "    val_subjs, val_ns = [], []\n",
    "    for key, desired_count in val_ns_files.items():\n",
    "        desired_n = int(key.split(\"_\")[1])\n",
    "        for n in range(desired_count):\n",
    "            try:\n",
    "                n_ix = sorted_ns.index(desired_n)\n",
    "            except ValueError:\n",
    "                raise ValueError((\n",
    "                    f\"Couldn't find subject {n} with desired count {desired_count} in \"\n",
    "                    \"the files left over for validation split. Please reduce the requested\"\n",
    "                    \" number of subjects for this desired count and re-run.\"\n",
    "                ))\n",
    "\n",
    "            val_subjs.append(sort_subjs.pop(n_ix))\n",
    "            val_ns.append(sorted_ns.pop(n_ix)) \n",
    "            n_val_files -= val_ns[-1]\n",
    "\n",
    "    while n_val_files>0:\n",
    "        val_subjs.append(sort_subjs.pop())\n",
    "        n_val_files -= sorted_ns.pop()\n",
    "\n",
    "    if len(val_subjs)>n_val_subjs:\n",
    "        print(\"Too many subjects selected based in initial passthrough. Attempting to fix...\")\n",
    "        while n_files_tol>0:\n",
    "            # popping from end, which removes subjects with 1 data file first\n",
    "            sort_subjs.append(val_subjs.pop())\n",
    "            sorted_ns.append(val_ns.pop())\n",
    "            n_files_tol -= sorted_ns[-1]\n",
    "\n",
    "        if len(val_subjs)>n_val_subjs:\n",
    "            raise ValueError((\n",
    "                f\"Cannot reconcile requirements for validation subject counts having \"\n",
    "                f\"particular numbers of data files with the constraints on number of \"\n",
    "                f\"validation subjects {n_val_subjs}, number of total validation files \"\n",
    "                f\"{n_val_files} and validation file count tolerance {n_files_tol}. Please\"\n",
    "                f\" either adjust subject counts by number of data files, the total number\"\n",
    "                f\" of desired validation files, or increase file count tolerance.\"))\n",
    "        else:\n",
    "            print(\"Fixed total-validation subject constraint given file-count tolerance...\")\n",
    "    elif len(val_subjs)<n_val_subjs:\n",
    "        print(\"Not enough subjects selected based in initial passthrough. Attempting to fix...\")\n",
    "\n",
    "        while n_files_tol>0:\n",
    "            # popping from end, which removes subjects with 1 data file first\n",
    "            val_subjs.append(sort_subjs.pop())\n",
    "            val_ns.append(sorted_ns.pop()) \n",
    "            n_files_tol -= val_ns[-1]\n",
    "\n",
    "        if len(val_subjs)>n_val_subjs:\n",
    "            raise ValueError((\n",
    "                f\"Cannot reconcile requirements for validation subject counts having \"\n",
    "                f\"particular numbers of data files with the constraints on number of \"\n",
    "                f\"validation subjects {n_val_subjs}, number of total validation files \"\n",
    "                f\"{n_val_files} and validation file count tolerance {n_files_tol}. Please\"\n",
    "                f\" either adjust subject counts by number of data files, the total number\"\n",
    "                f\" of desired validation files, or increase file count tolerance.\"))\n",
    "        else:\n",
    "            print(\"Fixed total-validation subject constraint given file-count tolerance...\")\n",
    "\n",
    "    # now, find the file names for each subject in val_subjs and sort_subjs (which is now\n",
    "    # just training subjects) and store as dict\n",
    "    train_val_dict= {\n",
    "        \"validation\": [file for subj in val_subjs for file in files_dict[subj]],\n",
    "        \"train\": [file for subj in sort_subjs for file in files_dict[subj] ],\n",
    "    }\n",
    "\n",
    "\n",
    "    # now, write dict to json\n",
    "    with open(\"../../src/data/train_val.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(train_val_dict, outfile)\n",
    "\n",
    "    return train_val_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files_tol=1\n",
    "desired_val_subjs = 15\n",
    "desired_val_files = 20\n",
    "train_val_dict = make_train_val_split_json(\"../../src/data/train-val_test.json\", desired_val_subjs, desired_val_files, n_files_tol=n_files_tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the data is split correctly. \n",
    "\n",
    "(1) If we want 1 subject each with 4, 3, and 2 data files, respectively, then there should be:\n",
    "- 1 data file with \"dataID3\" in the file name,\n",
    "- two data files with \"dataID2\" in the file name, and\n",
    "- three data files with \"dataID1\" in the file name\n",
    "\n",
    "(2) there should also be data from 15 subjects in the validation set.\n",
    "\n",
    "(3) there should be 20 (+/- 1) total validation files (3).\n",
    "\n",
    "(4) there should be no overlap between files in validation and training\n",
    "\n",
    "(5) there should be no duplicate files in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "nd3 = sum(\"dataID3\" in file for file in train_val_dict[\"validation\"])\n",
    "nd2 = sum(\"dataID2\" in file for file in train_val_dict[\"validation\"])\n",
    "nd1 = sum(\"dataID1\" in file for file in train_val_dict[\"validation\"])\n",
    "\n",
    "# these checks will need to be manually changed if we change the default values for \n",
    "# kwargs in make_train_val_split_json()\n",
    "assert nd3==1, \"Too many subjects with 4 data files\"\n",
    "assert nd2==2, \"Too many subjects with 3 data files\"\n",
    "assert nd1==3, \"Too many subjects with 2 data files\"\n",
    "\n",
    "# 2\n",
    "patt = re.compile(\"(subjID\\d+_)\")\n",
    "n_subjs = len(set([patt.findall(file)[0] for file in train_val_dict[\"validation\"]]))\n",
    "\n",
    "assert n_subjs==desired_val_subjs, \"Too many subjects in validation set\"\n",
    "\n",
    "# 3\n",
    "n_files = len(train_val_dict[\"validation\"])\n",
    "assert abs(n_files - desired_val_files) <= n_files_tol, \"Total validation file count not within tolerance of desired number\"\n",
    "\n",
    "# 4\n",
    "n_overlap = sum(\n",
    "    [val_file == train_file for val_file in train_val_dict[\"validation\"] for train_file in train_val_dict[\"train\"]]\n",
    ")\n",
    "assert n_overlap==0, \"Overlapping files between train and validations sets\"\n",
    "\n",
    "# 5\n",
    "n_dupl = sum(\n",
    "    [\n",
    "        train_val_dict[\"train\"][ix1] == train_val_dict[\"train\"][ix2] \n",
    "        for ix1 in range(len(train_val_dict[\"train\"])) \n",
    "        for ix2 in range(ix1+1, len(train_val_dict[\"train\"]))\n",
    "    ]\n",
    ")\n",
    "assert n_dupl==0, \"Duplicated file names in training set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions check out, so we are good to move forward..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('exercise_prediction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bc9e17ecfe5fe569855cd568414bc1d32da1f1775abbcdcf2e86e1b237526b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
