{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4-agifford-DetermineAnalysisWindowSize\n",
    "This notebook cycles through the data files in the training set in order to determine the \"ideal\" window size with which to calcuate the frequency features among labels. Essentially, we want sufficient frequency resolution to extract the whole-number-frequency features we decided on in 1.3, a sufficient number of training samples for model building, and sufficient precision in the estimates of magnitudes at each frequency feature. The trade off will be a smaller window size for more training samples vs. a larger window for better frequency resolution and better estimates of spectral magnitudes.\n",
    "\n",
    "Since we are working with whole-number frequencies as features, minimally we need 51 data points per sample to compute the spectra (since Fs = 50 Hz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft, fftshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../src/data/train_val_files.json\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    train_val_files = json.load(infile)\n",
    "train_files = train_val_files[\"train\"]\n",
    "\n",
    "with open(\"../../src/features/frequency_features.json\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    FEATURES = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we only want to compute the power for subsets of frequencies (rather than the entire FFT), it MAY be faster to do a direct calculation of just these frequency features, rather than doing a full FFT and then selecting out the frequencies of interest. Apparently, there is an algorithm to do that (the [Goertzel algorithm](https://en.wikipedia.org/wiki/Goertzel_algorithm)), but since I am not familiar with this algorithm I will simply compute the FFT and select the frequencies of interest for the sake of moving the project forward.\n",
    "\n",
    "Future work can explore whether the Goertzel algorithm would allow for faster feature calculations, which may be necessary for true \"real-time\" detection of exercise activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating functions here to compute the FFT across all measurements in one go rather than\n",
    "# looping through each\n",
    "\n",
    "# for this function, it should be sufficient to just calculate the raw FFT rather than\n",
    "# the normalized one since we only needed the normalized form to identify peaks above a\n",
    "# threshold\n",
    "def calculate_frequencies(n_fft, fs=50):\n",
    "    n_points = 2 * int(np.floor(n_fft / 2))\n",
    "    if n_fft % 2:\n",
    "        n_points += 1\n",
    "    freq = fs/2 * np.linspace(-1, 1, n_points)\n",
    "    return freq\n",
    "\n",
    "def calculate_spectrum(ndarray, fs=50):\n",
    "    X_w = fftshift(fft(WINDOW * ndarray, axis=0), axes=0)\n",
    "    \n",
    "    return np.abs(X_w / numpy.matlib.repmat(abs(X_w).max(axis=0), ndarray.shape[0], 1))\n",
    "\n",
    "def calculate_normed_spectrum(ndarray, fs=50):\n",
    "    n_fft = df.shape[0]\n",
    "    window = numpy.matlib.repmat(signal.hann(n_fft), 1, ndarray.shape[1])\n",
    "    X_w = fft(window * ndarray)\n",
    "    X_w_norm = 20 * np.log10(\n",
    "        np.abs(fftshift(X_w / numpy.matlib.repmat(abs(X_w).max(axis=0), ndarray.shape[0], 1)))\n",
    "    )\n",
    "\n",
    "    n_points = 2 * int(np.floor(n_fft / 2))\n",
    "    if n_fft % 2:\n",
    "        n_points += 1\n",
    "    freq = fs/2 * np.linspace(-1, 1, n_points)\n",
    "    return X_w_norm, freq\n",
    "\n",
    "def get_frequencies_indices(all_freqs, desired_freqs):\n",
    "    closest_freqs_ix = np.array([(np.abs([af - df for af in all_freqs])).argmin() for df in desired_freqs])\n",
    "    return closest_freqs_ix\n",
    "\n",
    "def calculate_windowed_feats(df, freq, cols = None):\n",
    "    cols = (\n",
    "        cols or [\"accel_x\", \"accel_y\", \"accel_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "    )\n",
    "    freq_ixs = [\n",
    "        get_frequencies_indices(freq, FEATURES[col]) for col in cols\n",
    "    ]\n",
    "    feat_cols = [\n",
    "        col + \"_\" + str(int(freq_feat)) for col in cols for freq_feat in FEATURES[col]\n",
    "    ]\n",
    "    \n",
    "    results_df = pd.DataFrame(columns=[\"file_id\", \"subject_id\", \"data_id\", \"window\", \"t_index\", \"label\", \"label_group\", *feat_cols])\n",
    "    for ix, t_start in enumerate(range(0, df.shape[0], N_FFT)):\n",
    "        if t_start + N_FFT > df.shape[0]-1:\n",
    "            continue\n",
    "\n",
    "        df_snip = df.loc[t_start:(t_start + N_FFT - 1), :].reset_index()\n",
    "\n",
    "        # create new label in case the time period encapsulates parts of multiple activity groups\n",
    "        if df_snip.label.nunique()>1:\n",
    "            label = \"Transition\"\n",
    "        else:\n",
    "            label = df_snip.loc[0, \"label\"]\n",
    "\n",
    "        if df_snip.label_group.nunique()>1:\n",
    "            label_group = \"Transition\"\n",
    "        else:\n",
    "            label_group = df_snip.loc[0, \"label_group\"]\n",
    "\n",
    "        # compute the spectra\n",
    "        X_w = calculate_spectrum(df_snip[cols].values)\n",
    "\n",
    "        # get only the desired frequencies by measurement\n",
    "        meas_feats = [\n",
    "            X_w[f_ix, ix] for ix, f_ix in enumerate(freq_ixs)\n",
    "        ]\n",
    "        # flatten the features to have same shape as feat_cols\n",
    "        flat_feats = [\n",
    "            f_data for col in meas_feats for f_data in col\n",
    "        ]\n",
    "        data = {\n",
    "            \"file_id\": df.loc[0, \"file_id\"], \n",
    "            \"subject_id\": df.loc[0, \"subject_id\"], \n",
    "            \"data_id\": df.loc[0, \"data_id\"], \n",
    "            \"window\": [N_FFT],\n",
    "            \"t_index\": [ix],\n",
    "            \"label\": [label],\n",
    "            \"label_group\": [label_group],\n",
    "            **{\n",
    "                key: [val] for key, val in zip(feat_cols, flat_feats)\n",
    "            }\n",
    "        }\n",
    "        window_df = pd.DataFrame(data=data)\n",
    "        results_df = pd.concat([results_df, window_df], ignore_index=True)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def agg_statistic(df, agg_col, statistic):\n",
    "    gb = df.groupby(agg_col, as_index=False).agg({\n",
    "        col: statistic for col in feats_only.columns[2:]\n",
    "    })\n",
    "    gb = gb.rename(columns={agg_col: \"activity\"})\n",
    "    gb[\"statistic\"] = statistic\n",
    "    return gb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FS = 50\n",
    "n_ffts = [(FS * r) + 1 for r in range(1, 7)] # for 1, 1.5, 2, 2.5, 3-s analysis windows\n",
    "cols = [\"accel_x\", \"accel_y\", \"accel_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "feat_cols = [\n",
    "    col + \"_\" + str(int(freq_feat)) for col in cols for freq_feat in FEATURES[col]\n",
    "]\n",
    "\n",
    "grand_results_df = pd.DataFrame(\n",
    "    columns = [\n",
    "        \"activity\",\n",
    "        *feat_cols,\n",
    "        \"statistic\",\n",
    "        \"file\",\n",
    "        \"window\",\n",
    "        \"count\",\n",
    "        \"label_type\"\n",
    "    ]\n",
    ")\n",
    "for ix, file in enumerate(train_files):\n",
    "    print(f\"analyzing file {ix+1} of {len(train_files)}...window samples: \", end=\"\")\n",
    "    df = pd.read_parquet(file, engine=\"fastparquet\")\n",
    "    for N_FFT in n_ffts:\n",
    "        print(f\"{N_FFT:03}, \", end=\"\")\n",
    "\n",
    "        WINDOW = numpy.matlib.repmat(signal.hann(N_FFT), 6, 1).T  # 6 for 6 measurement columns\n",
    "        freq = calculate_frequencies(N_FFT)\n",
    "        windowed_df = calculate_windowed_feats(df, freq)\n",
    "\n",
    "        # want mean and std of frequency features by label, label_group for each N_FFT\n",
    "        feats_only = windowed_df.drop(columns=[\"file_id\", \"subject_id\", \"data_id\", \"window\", \"t_index\"])\n",
    "\n",
    "        label_mn = agg_statistic(feats_only, \"label\", \"mean\")\n",
    "        label_std = agg_statistic(feats_only, \"label\", \"std\")\n",
    "        group_mn = agg_statistic(feats_only, \"label_group\", \"mean\")\n",
    "        group_std = agg_statistic(feats_only, \"label_group\", \"std\")\n",
    "\n",
    "        count = feats_only.shape[0]\n",
    "        label_mn[\"file\"], label_std[\"file\"], group_mn[\"file\"], group_std[\"file\"] = (\n",
    "            file, file, file, file\n",
    "        )\n",
    "        label_mn[\"window\"], label_std[\"window\"], group_mn[\"window\"], group_std[\"window\"] = (\n",
    "            N_FFT, N_FFT, N_FFT, N_FFT\n",
    "        )\n",
    "        label_mn[\"count\"], label_std[\"count\"], group_mn[\"count\"], group_std[\"count\"] = (\n",
    "            count, count, count, count\n",
    "        )\n",
    "        label_mn[\"label_type\"], label_std[\"label_type\"], group_mn[\"label_type\"], group_std[\"label_type\"] = (\n",
    "            \"label\", \"label\", \"label_group\", \"label_group\"\n",
    "        )\n",
    "        windowed_stats_df = pd.concat(\n",
    "            [label_mn, label_std, group_mn, group_std],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        grand_results_df = pd.concat([grand_results_df, windowed_stats_df], ignore_index=True)\n",
    "\n",
    "    print(\"\\r\", end=\"\")\n",
    "    print(\"\\r\", end=\"\")\n",
    "    print(\"\\r\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_label_mn = grand_results_df[\n",
    "    (grand_results_df[\"label_type\"] == \"label\") &\n",
    "    (grand_results_df[\"statistic\"] == \"mean\")\n",
    "]\n",
    "grand_label_std = grand_results_df[\n",
    "    (grand_results_df[\"label_type\"] == \"label\") &\n",
    "    (grand_results_df[\"statistic\"] == \"std\")\n",
    "]\n",
    "\n",
    "grand_group_mn = grand_results_df[\n",
    "    (grand_results_df[\"label_type\"] == \"label_group\") &\n",
    "    (grand_results_df[\"statistic\"] == \"mean\")\n",
    "]\n",
    "grand_group_std = grand_results_df[\n",
    "    (grand_results_df[\"label_type\"] == \"label_group\") &\n",
    "    (grand_results_df[\"statistic\"] == \"std\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we can get a minimum of 542,000 samples if we use a 3-second time bin for analyzing frequency features. Even this \"minimal\" amount should be enough for modeling. But more data is better, so let's check what the precision of data features are across data points to see whether a shorter time window provides sufficient precision,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_label_mn.groupby(\"window\").agg(NSamples=(\"count\", \"sum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the grand mean powers for all frequencies across window sizes is fairly consistent, with stds generally less than 0.1. So far so good for all window sizes in estimating mean power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(grand_label_mn.groupby(\"window\").agg({\n",
    "    col: \"mean\" for col in grand_label_mn.columns[1:66]\n",
    "}).std()>0.1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that most often, the 3-second time window had the smallest std of the mean values, followed (curiously) by the 1-second std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_label_mn.groupby(\"window\").agg({\n",
    "    col: \"std\" for col in grand_label_mn.columns[1:66]\n",
    "}).idxmin(axis=0).reset_index().groupby(0).agg(Count=(0, \"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the 3-second analysis window generally had the smallest average std across time bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_label_std.groupby(\"window\").agg({\n",
    "    col: \"mean\" for col in grand_label_mn.columns[1:66]\n",
    "}).idxmin(axis=0).reset_index().groupby(0).agg(Count=(0, \"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results apply at the level of `label_group`. Thus, in terms of estimated accuracy and precision of power estimates, the 3-second time window seems the best. This is what I will use moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(grand_group_mn.groupby(\"window\").agg({\n",
    "    col: \"mean\" for col in grand_group_mn.columns[1:66]\n",
    "}).std()>0.1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_group_mn.groupby(\"window\").agg({\n",
    "    col: \"std\" for col in grand_group_mn.columns[1:66]\n",
    "}).idxmin(axis=0).reset_index().groupby(0).agg(Count=(0, \"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_group_std.groupby(\"window\").agg({\n",
    "    col: \"mean\" for col in grand_group_mn.columns[1:66]\n",
    "}).idxmin(axis=0).reset_index().groupby(0).agg(Count=(0, \"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** There are definitely more sophisticated ways to assess the appropriate time window in which to analyze the data to detect exercises, in addition to other considerations (e.g., how real-time the classification should be). For the sake of simplicity and moving the project forward, I am leaving more detailed and sohpisticated methods for future versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('exercise_prediction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bc9e17ecfe5fe569855cd568414bc1d32da1f1775abbcdcf2e86e1b237526b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
