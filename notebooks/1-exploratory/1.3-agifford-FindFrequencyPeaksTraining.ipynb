{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3-agifford-FindFrequencyPeaksTraining\n",
    "This notebooks cycles through the training dataset to identify peak frequencies by activity label that cross a pre-determined threshold. We will adjust the threshold manually to ensure that each label only contributes ~1-2 frequencies to the feature set (not include 0 Hz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft, fftshift\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_single_annot_frame(df, shift):\n",
    "    annot_df = df[df.label != df.label.shift(shift)]\n",
    "    annot_df = annot_df.dropna(subset=\"label\").reset_index()\n",
    "    return annot_df\n",
    "\n",
    "def make_annot_dataframe(df, t_start=None, t_end=None):\n",
    "    t_start = t_start or df.time.min()\n",
    "    t_end = t_end or df.time.max()\n",
    "    \n",
    "    df = df[(df.time >= t_start) & (df.time <= t_end)].copy()\n",
    "    \n",
    "    (act_starts_df, act_ends_df) = (\n",
    "        _make_single_annot_frame(df, shift) for shift in [1, -1]\n",
    "    )\n",
    "    return act_starts_df, act_ends_df\n",
    "\n",
    "def local_fmax_above_thresh(freq, x_w, threshold):\n",
    "    local_max_ix = (np.diff(np.sign(np.diff(x_w))) < 0).nonzero()[0] + 1\n",
    "    x_w_max = x_w[local_max_ix]\n",
    "    freq_max = freq[local_max_ix]\n",
    "\n",
    "    return freq_max[np.where((x_w_max>threshold) & (freq_max>0))]\n",
    "\n",
    "def calculate_frequencies(n_fft, fs=50):\n",
    "    n_points = 2 * int(np.floor(n_fft / 2))\n",
    "    if n_fft % 2:\n",
    "        n_points += 1\n",
    "    freq = fs/2 * np.linspace(-1, 1, n_points)\n",
    "    return freq\n",
    "\n",
    "def calculate_normed_spectrum(df, col, fs=50):\n",
    "    n_fft = df.shape[0]\n",
    "    window = signal.hann(n_fft)\n",
    "    X_w = fft(window * df[col].values)\n",
    "    X_w_norm = 20 * np.log10(np.abs(fftshift(X_w / abs(X_w).max())))\n",
    "    return X_w_norm\n",
    "\n",
    "def _round(local_fmax, round_level):\n",
    "    return np.round(local_fmax * round_level) / round_level\n",
    "\n",
    "\n",
    "def find_single_file_peaks(df, thresholds, round_level):\n",
    "    activity_starts_df, activity_ends_df = make_annot_dataframe(df)\n",
    "    df_ =  df.dropna(subset=\"label\")\n",
    "\n",
    "    all_pks_df = pd.DataFrame(columns=[\"file_id\", \"subject_id\", \"data_id\", \"threshold\", \"label\", \"label_group\", \"peak_fs\"])\n",
    "    data_cols = [\"accel_x\", \"accel_y\", \"accel_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "    for thresh in thresholds:\n",
    "        for r_ix in range(activity_starts_df.shape[0]):\n",
    "            act_tp_df = pd.concat([activity_starts_df.loc[[r_ix], :], activity_ends_df.loc[[r_ix], :]], ignore_index=True)\n",
    "            df_snip = df_[(df_.time >= act_tp_df.loc[0, \"time\"]) & (df_.time <= act_tp_df.loc[1, \"time\"])].reset_index()\n",
    "            X_w_data = [calculate_normed_spectrum(df_snip, col) for col in data_cols]\n",
    "            freq = calculate_frequencies(len(X_w_data[0]))\n",
    "\n",
    "            rounded_fmax_data = []\n",
    "            col_data = []\n",
    "            for col, X_w in zip(data_cols, X_w_data):\n",
    "                local_fmax = local_fmax_above_thresh(freq, X_w, thresh)\n",
    "                rounded_fmax = _round(local_fmax, round_level)\n",
    "                rounded_fmax_data.extend(rounded_fmax)\n",
    "                col_data.extend([col for _ in rounded_fmax])\n",
    "\n",
    "            data = {\n",
    "                \"file_id\": [df_snip.loc[0, \"file_id\"] for _ in rounded_fmax_data],\n",
    "                \"subject_id\": [df_snip.loc[0, \"subject_id\"] for _ in rounded_fmax_data],\n",
    "                \"data_id\": [df_snip.loc[0, \"data_id\"] for _ in rounded_fmax_data],\n",
    "                \"threshold\": [thresh for _ in rounded_fmax_data],\n",
    "                \"label\": [act_tp_df.loc[0, \"label\"] for _ in rounded_fmax_data],\n",
    "                \"label_group\": [act_tp_df.loc[0, \"label_group\"] for _ in rounded_fmax_data],\n",
    "                \"measure\": col_data,\n",
    "                \"peak_fs\": rounded_fmax_data\n",
    "            }\n",
    "            pks_df = pd.DataFrame(data=data)\n",
    "            all_pks_df = pd.concat([all_pks_df, pks_df], ignore_index=True)\n",
    "    return all_pks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../src/data/train_val_files.json\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    train_val_files = json.load(infile)\n",
    "train_files = train_val_files[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [-5, -10, -15, -20]\n",
    "round_level = 1\n",
    "all_files_pks_df = pd.DataFrame(columns=[\"file_id\", \"subject_id\", \"data_id\", \"threshold\", \"label\", \"label_group\", \"measure\", \"peak_fs\"])\n",
    "for ix, file in enumerate(train_files):\n",
    "    print(f\"analyzing file {ix+1} of {len(train_files)}\", end=\"\\r\")\n",
    "    df = pd.read_parquet(file, engine=\"fastparquet\")\n",
    "    all_pks_df = find_single_file_peaks(df, thresholds, round_level)\n",
    "    all_files_pks_df = pd.concat([all_files_pks_df, all_pks_df], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_pks_df.groupby([\"threshold\", \"measure\", \"label\"], as_index=False).agg(count=(\"peak_fs\", \"nunique\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even with rounding to whole frequencies and the most restrictive frequency threshold, some labels have many \"significant peaks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npeaks_by_thresh_label = all_files_pks_df.groupby(\n",
    "    [\"threshold\", \"measure\", \"label\"], as_index=False\n",
    ").agg(\n",
    "    UniquePeaks=(\"peak_fs\", \"nunique\"), MinFreq=(\"peak_fs\", \"min\")\n",
    ").sort_values(by=[\"measure\", \"UniquePeaks\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npeaks_by_thresh_label[(npeaks_by_thresh_label[\"threshold\"]==-5) & (npeaks_by_thresh_label[\"UniquePeaks\"]>3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, I think I will keep the -5 threshold and \"fix\" the labels with >3 peak frequencies to select \"ideal\" (determined arbitrarily at this point) subsets. I will try to find the 2 peaks (besides 0 Hz) that are most differentiating to each of the labels compared to the other labels (for each measure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = [\"accel_x\", \"accel_y\", \"accel_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "for col in data_cols:\n",
    "    print(col)\n",
    "    print(\n",
    "        npeaks_by_thresh_label[\n",
    "            (npeaks_by_thresh_label[\"threshold\"]==-5) & \n",
    "            (npeaks_by_thresh_label[\"UniquePeaks\"]>=3) &\n",
    "            (npeaks_by_thresh_label[\"measure\"]==col)\n",
    "        ].shape[0]\n",
    "    )\n",
    "    print(\n",
    "        npeaks_by_thresh_label[\n",
    "            (npeaks_by_thresh_label[\"threshold\"]==-5) & \n",
    "            (npeaks_by_thresh_label[\"UniquePeaks\"]==2) &\n",
    "            (npeaks_by_thresh_label[\"measure\"]==col)\n",
    "        ].shape[0]\n",
    "    )\n",
    "    print(\n",
    "        npeaks_by_thresh_label[\n",
    "            (npeaks_by_thresh_label[\"threshold\"]==-5) & \n",
    "            (npeaks_by_thresh_label[\"UniquePeaks\"]==1) &\n",
    "            (npeaks_by_thresh_label[\"measure\"]==col)\n",
    "        ].shape[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the labels that have only one peak, is it the case that it is always 0 Hz? -> For all but `gyro_z`, this is the case. At least one label with only a single peak frequency has its peak at 1 Hz rather than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_thresh_pks = npeaks_by_thresh_label[npeaks_by_thresh_label[\"threshold\"] == -5]\n",
    "for col in data_cols:\n",
    "    print(col)\n",
    "    one_peak_labels = select_thresh_pks[\n",
    "        (select_thresh_pks[\"UniquePeaks\"]==1) &\n",
    "        (select_thresh_pks[\"measure\"]==col)\n",
    "    ]\n",
    "    print(one_peak_labels.MinFreq.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_thresh_pks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_pks_df = all_files_pks_df.drop(columns=[\"UniquePeaks\"], errors=\"ignore\")\n",
    "thresh_files_pks_df = all_files_pks_df.merge(select_thresh_pks.iloc[:, :4], on=[\"threshold\", \"measure\", \"label\"])\n",
    "select_labels_df = thresh_files_pks_df[thresh_files_pks_df.UniquePeaks >= 3]\n",
    "# don't need to analyze 0Hz since we're keeping it regardless\n",
    "select_labels_df = select_labels_df[select_labels_df.peak_fs > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frqs_by_activity = select_labels_df.groupby([\"label\", \"measure\"], as_index=False).agg(Peaks=(\"peak_fs\", set))\n",
    "\n",
    "other_labels_df = thresh_files_pks_df[thresh_files_pks_df.UniquePeaks < 3]\n",
    "# don't need to analyze 0Hz since we're keeping it regardless\n",
    "# other_labels_df = other_labels_df[other_labels_df.peak_fs > 0]\n",
    "frqs_by_other = other_labels_df[other_labels_df.peak_fs > 0].groupby([\"label\", \"measure\"], as_index=False).agg(Peaks=(\"peak_fs\", set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the labels x measures with only 1 or 2 peak frequencies (and after removing 0 Hz from all), we see that the overwhelming majority of `labels x measures` include 1 Hz (98%) and consist ONLY of 1 Hz (97%). However, there are some `labels x measures` whose other peak frequency is something other than 1 Hz. This tells me that I should not use 1 Hz for any of the labels in `frqs_by_activity` that include it (unless there are no other options). It also tells me I can generally ignore these other labels when comparing the unique frequencies for labels in `frqs_by_activity` to determine the most discriminating frequencies among the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of all rows that have 1 Hz as a peak frequency\n",
    "frqs_by_other[\"Peaks\"].apply(lambda x: 1 in x).sum() / frqs_by_other.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of all rows that have only 1 Hz as the peak frequency\n",
    "frqs_by_other[\"Peaks\"].apply(lambda x: {1} == x).sum() / frqs_by_other.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all possible other peak frequencies\n",
    "frqs_by_other[\"Peaks\"].apply(lambda x: max(x)).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for each `label`, I am searching for how often each `peak_fs` is differentiating among the other labels. For simplicity, I'm only comparing the peak frequencies within a measurement type (e.g., comparing only \"accel_x\" peaks for discriminating frequencies among accel_x data across labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_uniqf = pd.DataFrame(columns=[\"label\", \"measure\", \"peak_fs\"])\n",
    "for col in data_cols:\n",
    "    print(col)\n",
    "    col_df = frqs_by_activity[frqs_by_activity[\"measure\"] == col].reset_index()\n",
    "    for ix, if_set in enumerate(col_df.Peaks.values):\n",
    "        ilabel = col_df.loc[ix, \"label\"]\n",
    "        for jx, jf_set in enumerate(col_df.Peaks.values):\n",
    "            if ix == jx:\n",
    "                continue\n",
    "\n",
    "            diff = list(if_set - jf_set)\n",
    "            idf = pd.DataFrame(data={\n",
    "                \"label\": [ilabel for _ in diff],\n",
    "                \"measure\": [col for _ in diff],\n",
    "                \"peak_fs\": diff\n",
    "            })\n",
    "            labels_uniqf = pd.concat([labels_uniqf, idf], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I aggregate by `label` and `peak_fs` to identify how often each frequency for each label was differentiating, and separately count how often each frequency showed up as a peak for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_freqs = labels_uniqf.groupby([\"label\", \"measure\", \"peak_fs\"], as_index=False).agg(DiffCount=(\"peak_fs\", \"count\"))\n",
    "sel_labels_frq_cnt = select_labels_df.groupby([\"label\", \"measure\", \"peak_fs\"], as_index=False).agg(Count=(\"peak_fs\", \"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_labels_frq_cnt = sel_labels_frq_cnt.drop(columns=[\"DiffCount\"], errors=\"ignore\")\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.merge(\n",
    "    best_freqs,\n",
    "    on=[\"label\", \"measure\", \"peak_fs\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here I am determining how often each frequency showed up as a peak frequency across all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_counts = sel_labels_frq_cnt.groupby([\"measure\", \"peak_fs\"], as_index=False).agg(FreqCount=(\"peak_fs\", \"count\"))\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.drop(columns=[\"FreqCount\"], errors=\"ignore\")\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.merge(\n",
    "    freq_counts,\n",
    "    on=[\"measure\", \"peak_fs\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I will try to not use 1 Hz frequencies as much as possible, I will find all rows where `peak_fs` == 1 and where `label x measure` has more than 2 peak frequencies. Then, for these rows, I will delete 1 Hz from the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_labels_nfreqs = sel_labels_frq_cnt.groupby([\"label\", \"measure\"], as_index=False).agg(NFreqs=(\"peak_fs\", \"count\"))\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.drop(columns=\"NFreqs\", errors=\"ignore\")\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.merge(\n",
    "    sel_labels_nfreqs,\n",
    "    on=[\"label\", \"measure\"]\n",
    ")\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt[\n",
    "    (sel_labels_frq_cnt[\"peak_fs\"] != 1) &\n",
    "    (sel_labels_frq_cnt[\"NFreqs\"] > 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I want to find now are the \"best\" 2 frequencies for each `label x measure`. Here, I am defining \"best\" as those frequencies that show up often for an individual label AND are highly differentiating among other labels. To combine these characteristics, I create a column \"DiscrimFactor\", which simply multiplies \"DiffCount\" by \"Count\". Then, we sort the dataframe in descending order by \"DiscrimFactor\" in order and grab the first 2 frequencies per label. To account for potential ties in this metric, we next sort by \"DiffCount\" (descending), \"Count\" (descending), and finally \"FreqCount\" (ascending). In this way, we find:\n",
    "1. first, the frequencies with the highest \"DiscrimFactor\"\n",
    "2. next, the frequencies that are most differentiating among the other labels (\"DiffCount\")\n",
    "3. next, the frequencies that are most common for the given label (\"label\")\n",
    "4. finally, the frequencies that are least common over all labels (\"FreqCount\") -> this last point ensures that, all other things being equal, the frequency I pick for a given label is the most likely to be different from the ones already selected in the other labels (given random chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_labels_frq_cnt[\"DiscrimFactor\"] = sel_labels_frq_cnt[\"DiffCount\"] * sel_labels_frq_cnt[\"Count\"]\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.sort_values(by=[\"label\", \"measure\", \"DiscrimFactor\", \"DiffCount\", \"Count\", \"FreqCount\"], ascending=[True, True, False, False, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_labels_frq_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_freqs_labels = sel_labels_frq_cnt.groupby([\"label\", \"measure\"]).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we didn't lose any labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_check = list(other_labels_df.label.unique())\n",
    "labels_check.extend(select_freqs_labels.label.unique())\n",
    "\n",
    "all_labels = thresh_files_pks_df.label.unique()\n",
    "\n",
    "assert all([any([label1 == label2 for label2 in all_labels])] for label1 in labels_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this checks out, let's combine the dataframes back and then select all of the unique frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only want the non-duplicated labels-frequency combinations\n",
    "all_select_freqs = other_labels_df.drop_duplicates(subset=[\"label\", \"measure\", \"peak_fs\"])[[\"label\", \"measure\", \"peak_fs\"]]\n",
    "all_select_freqs = pd.concat([all_select_freqs, select_freqs_labels[[\"label\", \"measure\", \"peak_fs\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, there are 65 frequencies, which means there will be 65 features included in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_feats_meas = all_select_freqs.groupby(\"measure\", as_index=False).agg(Frequencies=(\"peak_fs\", lambda x: list(set(x))), NFreqs=(\"peak_fs\", \"nunique\"))\n",
    "print(freq_feats_meas.NFreqs.sum())\n",
    "freq_feats_meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in freq_feats_meas.itertuples():\n",
    "    print(row.measure)\n",
    "    print(row.Frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write them to a JSON to store for later use in further EDA and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_features = {\n",
    "    row.measure: row.Frequencies\n",
    "    for row in freq_feats_meas.itertuples()\n",
    "}\n",
    "freq_feat_path = Path(\"../../src/features/frequency_features.json\")\n",
    "if not freq_feat_path.exists():\n",
    "    with open(freq_feat_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(frequency_features, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('exercise_prediction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bc9e17ecfe5fe569855cd568414bc1d32da1f1775abbcdcf2e86e1b237526b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
