{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3-agifford-FindFrequencyPeaksTraining\n",
    "This notebooks cycles through the training dataset to identify peak frequencies by activity label that cross a pre-determined threshold. We will adjust the threshold manually to ensure that each label only contributes ~1-2 frequencies to the feature set (not include 0 Hz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft, fftshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_single_annot_frame(df, shift):\n",
    "    annot_df = df[df.label != df.label.shift(shift)]\n",
    "    annot_df = annot_df.dropna(subset=\"label\").reset_index()\n",
    "    return annot_df\n",
    "\n",
    "def make_annot_dataframe(df, t_start=None, t_end=None):\n",
    "    t_start = t_start or df.time.min()\n",
    "    t_end = t_end or df.time.max()\n",
    "    \n",
    "    df = df[(df.time >= t_start) & (df.time <= t_end)].copy()\n",
    "    \n",
    "    (act_starts_df, act_ends_df) = (\n",
    "        _make_single_annot_frame(df, shift) for shift in [1, -1]\n",
    "    )\n",
    "    return act_starts_df, act_ends_df\n",
    "\n",
    "def local_fmax_above_thresh(freq, x_w, threshold):\n",
    "    local_max_ix = (np.diff(np.sign(np.diff(x_w))) < 0).nonzero()[0] + 1\n",
    "    x_w_max = x_w[local_max_ix]\n",
    "    freq_max = freq[local_max_ix]\n",
    "\n",
    "    return freq_max[np.where((x_w_max>threshold) & (freq_max>0))]\n",
    "\n",
    "def calculate_normed_spectrum(df, fs=50):\n",
    "    n_fft = df.shape[0]\n",
    "    window = signal.hann(n_fft)\n",
    "    X_w = fft(window * df.accel_x.values)\n",
    "    X_w_norm = 20 * np.log10(np.abs(fftshift(X_w / abs(X_w).max())))\n",
    "\n",
    "    n_points = 2 * int(np.floor(n_fft / 2))\n",
    "    if n_fft % 2:\n",
    "        n_points += 1\n",
    "    freq = fs/2 * np.linspace(-1, 1, n_points)\n",
    "    return X_w_norm, freq\n",
    "\n",
    "def _round(local_fmax, round_level):\n",
    "    return np.round(local_fmax * round_level) / round_level\n",
    "\n",
    "\n",
    "def find_single_file_peaks(df, thresholds, round_level):\n",
    "    activity_starts_df, activity_ends_df = make_annot_dataframe(df)\n",
    "    df_ =  df.dropna(subset=\"label\")\n",
    "\n",
    "    all_pks_df = pd.DataFrame(columns=[\"file_id\", \"subject_id\", \"data_id\", \"threshold\", \"label\", \"label_group\", \"peak_fs\"])\n",
    "    for thresh in thresholds:\n",
    "        for r_ix in range(activity_starts_df.shape[0]):\n",
    "            act_tp_df = pd.concat([activity_starts_df.loc[[r_ix], :], activity_ends_df.loc[[r_ix], :]], ignore_index=True)\n",
    "            df_snip = df_[(df_.time >= act_tp_df.loc[0, \"time\"]) & (df_.time <= act_tp_df.loc[1, \"time\"])].reset_index()\n",
    "            X_w_norm, freq = calculate_normed_spectrum(df_snip)\n",
    "\n",
    "            local_fmax = local_fmax_above_thresh(freq, X_w_norm, thresh)\n",
    "            rounded_fmax = _round(local_fmax, round_level)\n",
    "\n",
    "            data = {\n",
    "                \"file_id\": [df_snip.loc[0, \"file_id\"] for _ in rounded_fmax],\n",
    "                \"subject_id\": [df_snip.loc[0, \"subject_id\"] for _ in rounded_fmax],\n",
    "                \"data_id\": [df_snip.loc[0, \"data_id\"] for _ in rounded_fmax],\n",
    "                \"threshold\": [thresh for _ in rounded_fmax],\n",
    "                \"label\": [act_tp_df.loc[0, \"label\"] for _ in rounded_fmax],\n",
    "                \"label_group\": [act_tp_df.loc[0, \"label_group\"] for _ in rounded_fmax],\n",
    "                \"peak_fs\": rounded_fmax\n",
    "            }\n",
    "            pks_df = pd.DataFrame(data=data)\n",
    "            all_pks_df = pd.concat([all_pks_df, pks_df], ignore_index=True)\n",
    "    return all_pks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../src/data/train_val_files.json\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    train_val_files = json.load(infile)\n",
    "train_files = train_val_files[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [-5, -10, -15, -20]\n",
    "round_level = 1\n",
    "all_files_pks_df = pd.DataFrame(columns=[\"file_id\", \"subject_id\", \"data_id\", \"threshold\", \"label\", \"label_group\", \"peak_fs\"])\n",
    "for ix, file in enumerate(train_files):\n",
    "    print(f\"analyzing file {ix+1} of {len(train_files)}\", end=\"\\r\")\n",
    "    df = pd.read_parquet(file, engine=\"fastparquet\")\n",
    "    all_pks_df = find_single_file_peaks(df, thresholds, round_level)\n",
    "    all_files_pks_df = pd.concat([all_files_pks_df, all_pks_df], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even with rounding to whole frequencies and the most restrictive frequency threshold, some labels have many \"significant peaks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_pks_df[all_files_pks_df.peak_fs == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npeaks_by_thresh_label = all_files_pks_df.groupby(\n",
    "    [\"threshold\", \"label\"], as_index=False\n",
    ").agg(\n",
    "    UniquePeaks=(\"peak_fs\", \"nunique\"), MinFreq=(\"peak_fs\", \"min\")\n",
    ").sort_values(by=\"UniquePeaks\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npeaks_by_thresh_label[(npeaks_by_thresh_label[\"threshold\"]==-5) & (npeaks_by_thresh_label[\"UniquePeaks\"]>3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, I think I will keep the -5 threshold and \"fix\" the 11 labels with >3 peak frequencies to select an \"ideal\" (determined arbitrarily at this point) subset. I will try to find the 2 peaks (besides 0 Hz) that are most differentiating to each of the labels compared to the other labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(npeaks_by_thresh_label[(npeaks_by_thresh_label[\"threshold\"]==-5) & (npeaks_by_thresh_label[\"UniquePeaks\"]>=3)].shape[0])\n",
    "print(npeaks_by_thresh_label[(npeaks_by_thresh_label[\"threshold\"]==-5) & (npeaks_by_thresh_label[\"UniquePeaks\"]==2)].shape[0])\n",
    "print(npeaks_by_thresh_label[(npeaks_by_thresh_label[\"threshold\"]==-5) & (npeaks_by_thresh_label[\"UniquePeaks\"]==1)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the labels that have only one peak, is it the case that it is always 0 Hz? -> Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_thresh_pks = npeaks_by_thresh_label[npeaks_by_thresh_label[\"threshold\"] == -5]\n",
    "one_peak_labels = select_thresh_pks[select_thresh_pks[\"UniquePeaks\"]==1]\n",
    "one_peak_labels.MinFreq.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_pks_df = all_files_pks_df.drop(columns=[\"UniquePeaks\"], errors=\"ignore\")\n",
    "thresh_files_pks_df = all_files_pks_df.merge(select_thresh_pks.iloc[:, :3], on=[\"threshold\", \"label\"])\n",
    "select_labels_df = thresh_files_pks_df[thresh_files_pks_df.UniquePeaks >= 3]\n",
    "# don't need to analyze 0Hz since we're keeping it regardless\n",
    "select_labels_df = select_labels_df[select_labels_df.peak_fs > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frqs_by_activity = select_labels_df.groupby(\"label\", as_index=False).agg(Peaks=(\"peak_fs\", set))\n",
    "\n",
    "other_labels_df = thresh_files_pks_df[thresh_files_pks_df.UniquePeaks < 3]\n",
    "# don't need to analyze 0Hz since we're keeping it regardless\n",
    "# other_labels_df = other_labels_df[other_labels_df.peak_fs > 0]\n",
    "frqs_by_other = other_labels_df[other_labels_df.peak_fs > 0].groupby(\"label\", as_index=False).agg(Peaks=(\"peak_fs\", set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the peak frequencies (other than 0 Hz) for the \"other\" activities only includes 1 Hz tells me that all of the 2-peak activities are all 0 Hz and 1 Hz. This tells me that I should not use 1 Hz for any of the labels in `frqs_by_activity` that include it. It also tells me I can ignore these other labels when comparing the unique frequencies for labels in `frqs_by_activity` to determine the most discriminating frequencies among the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frqs_by_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for each `label`, I am searching for how often each `peak_fs` is differentiating among the other labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_uniqf = pd.DataFrame(columns=[\"label\", \"peak_fs\"])\n",
    "for ix, if_set in enumerate(frqs_by_activity.Peaks.values):\n",
    "    ilabel = frqs_by_activity.loc[ix, \"label\"]\n",
    "    for jx, jf_set in enumerate(frqs_by_activity.Peaks.values):\n",
    "        if ix == jx:\n",
    "            continue\n",
    "\n",
    "        diff = list(if_set - jf_set)\n",
    "        idf = pd.DataFrame(data={\n",
    "            \"label\": [ilabel for _ in diff],\n",
    "            \"peak_fs\": diff\n",
    "        })\n",
    "        labels_uniqf = pd.concat([labels_uniqf, idf], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I aggregate by `label` and `peak_fs` to identify how often each frequency for each label was differentiating, and separately count how often each frequency showed up as a peak for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_freqs = labels_uniqf.groupby([\"label\", \"peak_fs\"], as_index=False).agg(DiffCount=(\"peak_fs\", \"count\"))\n",
    "sel_labels_frq_cnt = select_labels_df.groupby([\"label\", \"peak_fs\"], as_index=False).agg(Count=(\"peak_fs\", \"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_labels_frq_cnt = sel_labels_frq_cnt.drop(columns=[\"DiffCount\"], errors=\"ignore\")\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.merge(\n",
    "    best_freqs,\n",
    "    on=[\"label\", \"peak_fs\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here I am determining how often each frequency showed up as a peak frequency across all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_counts = sel_labels_frq_cnt.groupby(\"peak_fs\", as_index=False).agg(FreqCount=(\"peak_fs\", \"count\"))\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.drop(columns=[\"FreqCount\"], errors=\"ignore\")\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.merge(\n",
    "    freq_counts,\n",
    "    on=[\"peak_fs\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I want to find are the \"best\" 2 frequencies for each label. Here, I am defining \"best\" as those frequencies that show up often for an individual label AND are highly differentiating among other labels. To combine these characteristics, I create a column \"DiscrimFactor\", which simply multiplies \"DiffCount\" by \"Count\". Then, we sort the dataframe in descending order by \"DiscrimFactor\" in order and grab the first 2 frequencies per label. To account for potential ties in this metric, we next sort by \"DiffCount\" (descending), \"Count\" (descending), and finally \"FreqCount\" (ascending). In this way, we find:\n",
    "1. first, the frequencies with the highest \"DiscrimFactor\"\n",
    "2. next, the frequencies that are most differentiating among the other labels (\"DiffCount\")\n",
    "3. next, the frequencies that are most common for the given label (\"label\")\n",
    "4. finally, the frequencies that are least common over all labels (\"FreqCount\") -> this last point ensures that, all other things being equal, the frequency I pick for a given label is the most likely to be different from the ones already selected in the other labels (given random chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_labels_frq_cnt[\"DiscrimFactor\"] = sel_labels_frq_cnt[\"DiffCount\"] * sel_labels_frq_cnt[\"Count\"]\n",
    "sel_labels_frq_cnt = sel_labels_frq_cnt.sort_values(by=[\"label\", \"DiscrimFactor\", \"DiffCount\", \"Count\", \"FreqCount\"], ascending=[True, False, False, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_freqs_labels = sel_labels_frq_cnt.groupby(\"peak_fs\").head(2).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we didn't lose any labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_check = list(other_labels_df.label.unique())\n",
    "labels_check.extend(select_freqs_labels.label.unique())\n",
    "\n",
    "all_labels = thresh_files_pks_df.label.unique()\n",
    "\n",
    "assert all([any([label1 == label2 for label2 in all_labels])] for label1 in labels_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this checks out, let's combine the dataframes back and then select all of the unique frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only want the non-duplicated labels-frequency combinations\n",
    "all_select_freqs = other_labels_df.drop_duplicates(subset=[\"label\", \"peak_fs\"])[[\"label\", \"peak_fs\"]]\n",
    "all_select_freqs = pd.concat([all_select_freqs, select_freqs_labels[[\"label\", \"peak_fs\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, there are 24 frequencies, which means there will be 24 features included in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_select_freqs.peak_fs.nunique(), all_select_freqs.peak_fs.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write them to a JSON to store for later use in further EDA and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_features = {\n",
    "    \"frequencies\": all_select_freqs.peak_fs.unique().tolist()\n",
    "}\n",
    "with open(\"../../src/features/frequency_features.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(frequency_features, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('exercise_prediction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bc9e17ecfe5fe569855cd568414bc1d32da1f1775abbcdcf2e86e1b237526b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
